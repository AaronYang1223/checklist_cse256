{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook documents test suites in Checklist. If you are not already familiar with creating tests in Checklist, consider reading the MFT Examples notebook.\n",
    "\n",
    "## Setup\n",
    "First, let's import the libraries and load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "import checklist\n",
    "from checklist.editor import Editor\n",
    "from checklist.expect import Expect\n",
    "from checklist.pred_wrapper import PredictorWrapper\n",
    "from checklist.test_types import MFT\n",
    "from checklist.test_suite import TestSuite\n",
    "from torch.nn import functional as F\n",
    "from typing import List\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7faa7904b110>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize random seed\n",
    "# Remove this code to experiment with random samples\n",
    "random.seed(123)\n",
    "torch.manual_seed(456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Model loaded'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pretrained model tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "# Load pretrained model (weights)\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n",
    "device = 'cuda'\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\"Model loaded\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Test Suite\n",
    "\n",
    "Checklist can run multiple tests in a test suite. Tests can be grouped by capability and results can be explored in a visual table.\n",
    "\n",
    "We will create a test suite called 'Same Token Prediction' with 3 MFTs. Each MFT will test if the token substituted into the prompt template also appears in the generated text.\n",
    "\n",
    "For example, if we prompt the model with \"The **dog** is running in the zoo\" and the model responds with \"The **dog** looks very happy\", then it passes the test because the same animal appears in the model's response.\n",
    "\n",
    "## Creating the MFTs\n",
    "### MFT 1: Same animal appears in response\n",
    "This MFT uses an `{animal}` placeholder in the template. The expectation function checks that the same animal appears in the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The dog is running in the zoo',\n",
       " 'The cat is running in the zoo',\n",
       " 'The giraffe is running in the zoo',\n",
       " 'The aardvark is running in the zoo']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor = Editor()\n",
    "animal_prompts = editor.template(\"The {animal} is running in the zoo\", animal=[\"dog\", \"cat\", \"giraffe\", \"aardvark\"], meta=True)\n",
    "animal_prompts.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_same_animal(x, pred, conf, label=None, meta=None):\n",
    "    return meta['animal'] in pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_animal_expect_fn = Expect.single(contains_same_animal)\n",
    "same_animal_test = MFT(**animal_prompts, name='Same animal in response', description='The response contains the same animal mentioned in the prompt.', expect=same_animal_expect_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MFT 2: Same country appears in response\n",
    "This MFT uses a `{country}` placeholder in the template. The expectation function checks that the same country appears in the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MunchWithAdd({'meta': [{'country': 'Tajikistan'}, {'country': 'Bolivia'}, {'country': 'Japan'}, {'country': 'Kiribati'}, {'country': 'Kyrgyzstan'}, {'country': 'Namibia'}, {'country': 'Malaysia'}, {'country': 'Honduras'}, {'country': 'Ukraine'}, {'country': 'Angola'}], 'data': ['Earlier today, scientists from Tajikistan discovered', 'Earlier today, scientists from Bolivia discovered', 'Earlier today, scientists from Japan discovered', 'Earlier today, scientists from Kiribati discovered', 'Earlier today, scientists from Kyrgyzstan discovered', 'Earlier today, scientists from Namibia discovered', 'Earlier today, scientists from Malaysia discovered', 'Earlier today, scientists from Honduras discovered', 'Earlier today, scientists from Ukraine discovered', 'Earlier today, scientists from Angola discovered']})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_prompts = editor.template(\"Earlier today, scientists from {country} discovered\", meta=True, nsamples=10)\n",
    "country_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_same_country(x, pred, conf, label=None, meta=None):\n",
    "    return meta['country'] in pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_country_expect_fn = Expect.single(contains_same_country)\n",
    "same_country_test = MFT(**country_prompts, name='Same country in response', description='The response contains the same country mentioned in the prompt.', expect=same_country_expect_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MFT 3: Same person appears in response\n",
    "This MFT uses a `{first_name}` placeholder in the template. The expectation function checks that the same first name appears in the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MunchWithAdd({'meta': [{'first_name': 'Marie'}, {'first_name': 'Ben'}, {'first_name': 'Jill'}, {'first_name': 'Jill'}, {'first_name': 'Andrew'}, {'first_name': 'Victoria'}, {'first_name': 'Philip'}, {'first_name': 'Charlie'}, {'first_name': 'Cynthia'}, {'first_name': 'Lawrence'}], 'data': ['Marie is my neighbor.', 'Ben is my neighbor.', 'Jill is my neighbor.', 'Jill is my neighbor.', 'Andrew is my neighbor.', 'Victoria is my neighbor.', 'Philip is my neighbor.', 'Charlie is my neighbor.', 'Cynthia is my neighbor.', 'Lawrence is my neighbor.']})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person_prompts = editor.template(\"{first_name} is my neighbor.\", meta=True, nsamples=10)\n",
    "person_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_same_person(x, pred, conf, label=None, meta=None):\n",
    "    return meta['first_name'] in pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_person_expect_fn = Expect.single(contains_same_person)\n",
    "same_person_test = MFT(**person_prompts, name='Same person in response', description='The response contains the same person\\'s first name mentioned in the prompt.', expect=same_person_expect_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the tests to the suite\n",
    "The `TestSuite()` constructor creates an empty test suite. Tests can be added one by one using `suite.add(test)`. The optional `capability` parameter can be used to label and group tests that test similar capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = TestSuite()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite.add(same_animal_test, capability=\"Same Token Prediction\")\n",
    "suite.add(same_country_test, capability=\"Same Token Prediction\")\n",
    "suite.add(same_person_test, capability=\"Same Token Prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the predictions\n",
    "Now we define the function that Checklist will use to generate predictions from the model. The predictions need to be returned in the form `([predictions], [scores])`, so we will wrap the `generate_sentences()` function with `PredictorWrapper.wrap_predict()` to automatically create a tuple `([predictions], [1, 1, ...])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(prompt: str) -> str:\n",
    "    token_tensor = tokenizer.encode(prompt, return_tensors='pt').to(device) # return_tensors = \"pt\" returns a PyTorch tensor\n",
    "    out = model.generate(\n",
    "        token_tensor,\n",
    "        do_sample=True,\n",
    "        min_length=10,\n",
    "        max_length=50,\n",
    "        num_beams=1,\n",
    "        temperature=1.0,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=False,\n",
    "        output_scores=True,\n",
    "        return_dict_in_generate=True)\n",
    "    text = tokenizer.decode(out.sequences[0], skip_special_tokens=True)\n",
    "    return text[len(prompt):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentences(prompts: List[str]) -> List[str]:\n",
    "    sentences = []\n",
    "    for prompt in prompts:\n",
    "        sentences.append(generate_sentence(prompt))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([' Now, let us begin to talk…\\n\\nI heard you guys were coming.\\n: I thought you were from there, right?\\n (Slight sigh, looks back at Vixen) Yeah,',\n",
       "  ' Goodbye, goodbye, dear.\" But when I heard that I felt almost lost.\\n\\nMy wife gave me the same answer of \"Thank you.\"\\n.'],\n",
       " array([1., 1.]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_generator = PredictorWrapper.wrap_predict(generate_sentences)\n",
    "wrapped_generator([\"Hello, nice to meet you.\", \"Goodbye, see you later.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the suite\n",
    "We can now run the suite and view the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Same animal in response\n",
      "Predicting 4 examples\n",
      "Running Same country in response\n",
      "Predicting 10 examples\n",
      "Running Same person in response\n",
      "Predicting 10 examples\n"
     ]
    }
   ],
   "source": [
    "suite.run(wrapped_generator, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_example(x, pred, conf, label=None, meta=None): \n",
    "    return 'Prompt:      %s\\nCompletion:      %s' % (x, pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same Token Prediction\n",
      "\n",
      "Same animal in response\n",
      "Test cases:      4\n",
      "Fails (rate):    4 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "Prompt:      The dog is running in the zoo\n",
      "Completion:      , she says, and a friend of her walks in on them, shouting things like, \"I was scared, can you please sit down\"?\n",
      "\n",
      "He's also apparently had a seizure and an eye scratch,\n",
      "----\n",
      "Prompt:      The cat is running in the zoo\n",
      "Completion:      , but I'll only bring back a one year old. There's one guy at the park that is crazy. He was doing some nice things but it's definitely going to end up being one of those stories that\n",
      "----\n",
      "Prompt:      The aardvark is running in the zoo\n",
      "Completion:      . I got to see it in person.\n",
      "\n",
      "Lately, my daughter, who was visiting from college now, said she's been fascinated with the penguin-shaped shell on the side of\n",
      "----\n",
      "\n",
      "\n",
      "Same country in response\n",
      "Test cases:      10\n",
      "Fails (rate):    10 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "Prompt:      Earlier today, scientists from Angola discovered\n",
      "Completion:       that black holes on average do not produce more fuel at high power levels than their white equivalents, which may imply that their fuel reserves are lower.\n",
      "----\n",
      "Prompt:      Earlier today, scientists from Malaysia discovered\n",
      "Completion:       what they thought would be its first \"human-like\" planet. And while they were still waiting for the results from their previous work with the alien races, they found other promising signs. In the new study,\n",
      "----\n",
      "Prompt:      Earlier today, scientists from Kiribati discovered\n",
      "Completion:       a new layer of oxygen in the ocean around Antarctica.\n",
      "\n",
      "This comes after NASA announced that Antarctic marine microbes could live on methane, a fuel element at the top of Arctic ice caps.[1]\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "Same person in response\n",
      "Test cases:      10\n",
      "Fails (rate):    8 (80.0%)\n",
      "\n",
      "Example fails:\n",
      "Prompt:      Andrew is my neighbor.\n",
      "Completion:       And he's my friend. So he knows every single day that I'm wrong.\"\n",
      "\n",
      "After the trial, Judge James A. McGinley told him he had ordered his girlfriend to remove his shirt and throw it in\n",
      "----\n",
      "Prompt:      Charlie is my neighbor.\n",
      "Completion:       He's such a sweet guy. His wife is great. I've never seen her this good. It's a small community. We had great neighbors.\"\n",
      "\n",
      "He went on to say, \"I think your husband and\n",
      "----\n",
      "Prompt:      Philip is my neighbor.\n",
      "Completion:       I met him a few months ago at a show called TONNIE, where we just had an idea of how we might fit together like brothers and we decided to pick out his character and do some cool stuff together\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "suite.summary(format_example_fn = format_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait as we prepare the table data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5e517dd34764a05b6edf04d198f70fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SuiteSummarizer(stats={'npassed': 0, 'nfailed': 0, 'nfiltered': 0}, test_infos=[{'name': 'Same animal in respo…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "suite.visual_summary_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using files with test suites\n",
    "\n",
    "Some models cannot be run directly on the same machine that is running the Checklist test suite. For instance, a model might need to run in a specially configured lab environment. In this case, Checklist does not have to receive the predictions from the model directly. The predictions can be saved to a file, then the test suite can check the predictions from the file.\n",
    "\n",
    "## Exporting a test suite to a file\n",
    "First, let's create a file that contains all the prompts that we will send to the model.\n",
    "\n",
    "### Accessing test suite data internally\n",
    "Tests are stored in `suite.tests`, which is a dictionary mapping the test name to the test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing test suite data internally\n",
    "Tests are stored in `suite.tests`, which is a dictionary mapping the test name to the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same animal in response\n",
      "Same country in response\n",
      "Same person in response\n"
     ]
    }
   ],
   "source": [
    "for key in suite.tests.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the test information by like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The dog is running in the zoo',\n",
       " 'The cat is running in the zoo',\n",
       " 'The giraffe is running in the zoo',\n",
       " 'The aardvark is running in the zoo']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suite.tests['Same animal in response'].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'animal': 'dog'},\n",
       " {'animal': 'cat'},\n",
       " {'animal': 'giraffe'},\n",
       " {'animal': 'aardvark'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suite.tests['Same animal in response'].meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting to JSON file with to_raw_file()\n",
    "TestSuite's `to_raw_file()` function exports a test suite to a file. The `format_fn` parameter allows us to control how each example in the suite is printed to the file. We can use `format_fn` to print the examples in a JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suite_to_json_file(suite, filename):\n",
    "    class Counter:\n",
    "        def __init__(self):\n",
    "            self.count = 0\n",
    "        def get_count(self):\n",
    "            self.count += 1\n",
    "            return self.count\n",
    "    \n",
    "    counter = Counter()\n",
    "    total_tests = 0\n",
    "    for t in suite.tests.values():\n",
    "        total_tests += len(t.data)\n",
    "        \n",
    "    def json_format_fn(x):\n",
    "        example_id = counter.get_count()\n",
    "        json_str = \"\"\n",
    "        if example_id == 1:\n",
    "            json_str = '{\"examples\": ['\n",
    "        json_str += json.dumps({'content': x, 'id': example_id}) + \",\"\n",
    "        if example_id == total_tests:\n",
    "            # remove trailing comma\n",
    "            json_str = json_str[:len(json_str)-1]\n",
    "            json_str += \"]}\"\n",
    "        return json_str\n",
    "    \n",
    "    suite.to_raw_file(filename, format_fn = json_format_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite_to_json_file(suite, 'same_token_suite.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"examples\": [{\"content\": \"The dog is running in the zoo\", \"id\": 1},\r\n",
      "{\"content\": \"The cat is running in the zoo\", \"id\": 2},\r\n",
      "{\"content\": \"The giraffe is running in the zoo\", \"id\": 3},\r\n",
      "{\"content\": \"The aardvark is running in the zoo\", \"id\": 4},\r\n",
      "{\"content\": \"Earlier today, scientists from Tajikistan discovered\", \"id\": 5},\r\n",
      "{\"content\": \"Earlier today, scientists from Bolivia discovered\", \"id\": 6},\r\n",
      "{\"content\": \"Earlier today, scientists from Japan discovered\", \"id\": 7},\r\n",
      "{\"content\": \"Earlier today, scientists from Kiribati discovered\", \"id\": 8},\r\n",
      "{\"content\": \"Earlier today, scientists from Kyrgyzstan discovered\", \"id\": 9},\r\n",
      "{\"content\": \"Earlier today, scientists from Namibia discovered\", \"id\": 10},\r\n",
      "{\"content\": \"Earlier today, scientists from Malaysia discovered\", \"id\": 11},\r\n",
      "{\"content\": \"Earlier today, scientists from Honduras discovered\", \"id\": 12},\r\n",
      "{\"content\": \"Earlier today, scientists from Ukraine discovered\", \"id\": 13},\r\n",
      "{\"content\": \"Earlier today, scientists from Angola discovered\", \"id\": 14},\r\n",
      "{\"content\": \"Marie is my neighbor.\", \"id\": 15},\r\n",
      "{\"content\": \"Ben is my neighbor.\", \"id\": 16},\r\n",
      "{\"content\": \"Jill is my neighbor.\", \"id\": 17},\r\n",
      "{\"content\": \"Jill is my neighbor.\", \"id\": 18},\r\n",
      "{\"content\": \"Andrew is my neighbor.\", \"id\": 19},\r\n",
      "{\"content\": \"Victoria is my neighbor.\", \"id\": 20},\r\n",
      "{\"content\": \"Philip is my neighbor.\", \"id\": 21},\r\n",
      "{\"content\": \"Charlie is my neighbor.\", \"id\": 22},\r\n",
      "{\"content\": \"Cynthia is my neighbor.\", \"id\": 23},\r\n",
      "{\"content\": \"Lawrence is my neighbor.\", \"id\": 24}]}"
     ]
    }
   ],
   "source": [
    "cat 'same_token_suite.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the test suite JSON\n",
    "The JSON file we created can be imported back into a Python object by using `json.load()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'The dog is running in the zoo', 'id': 1},\n",
       " {'content': 'The cat is running in the zoo', 'id': 2},\n",
       " {'content': 'The giraffe is running in the zoo', 'id': 3}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "f = open('same_token_suite.json', 'r')\n",
    "suite_dict = json.load(f)\n",
    "f.close()\n",
    "suite_dict['examples'][0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating predictions from the loaded data\n",
    "Our data has been loaded into a variable named `suite_dict`. Now we can read each example from `suite_dict` and generate the predictions. Each prediction will be written to another file named `same_token_suite_predictions.json`, which will be sent to Checklist to evaluate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('same_token_suite_predictions.json', 'w') as f:\n",
    "    for example in suite_dict['examples']:\n",
    "        prediction = generate_sentence(example['content'])\n",
    "        prediction = prediction.replace('\"', '\\\"')\n",
    "        f.write(json.dumps({'prediction': prediction, 'id': example['id']}) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"prediction\": \" to study the animals in a cage where it is kept. After taking the test and asking the other dog about its health, he takes the blood samples and looks at them again and again.\\n\\nIf the results\", \"id\": 1}\r\n",
      "{\"prediction\": \".\\n\\nThat's a small number (at the moment), but it's not the only cat to look out for the blind. Other animals also get attention from other animals. It's the reason we get the\", \"id\": 2}\r\n",
      "{\"prediction\": \" with six other giraffes. Credit: Dr Rolf Lecka/Flickr (CC BY 2.0)\\n\\nBut as the animal swells out of the pen and into adulthood, it will\", \"id\": 3}\r\n",
      "{\"prediction\": \", as he runs out during a lecture and gets caught up in it as well. He also finds out about the future of his mother.\\n\\nNotes\", \"id\": 4}\r\n",
      "{\"prediction\": \" a new method of extracting plutonium. The team showed that a series of two hydrogen-containing platinum atoms could be used as a nucleite to produce a plutonium-like molecule which would then be added to the\", \"id\": 5}\r\n",
      "{\"prediction\": \" one of the world's largest \\\"superfossil lakes,\\\" which were almost 70-feet long.\\n\\nThey discovered a 3-mile thick layer of sediment that could cover much of Mexico. These lakes are\", \"id\": 6}\r\n",
      "{\"prediction\": \" that there are two of these species of birds living in the northern hemisphere.\\n\\nAlthough both species are classified as endemic by U.S. Geological Survey (USGS), one of their primary habitats will be in\", \"id\": 7}\r\n",
      "{\"prediction\": \" that many of its planets align and are actually moving in the opposite direction of what astronomers expected. \\\"According to our predictions, Pluto's system of icy moons would be a distant gas giant, with only a\", \"id\": 8}\r\n",
      "{\"prediction\": \" that a species that normally lives in South Caucasus waters, known as the Spiny Sea Beetle group, has been found in an area of Siberia's north-central Caspian Sea.\\n\\nThis\", \"id\": 9}\r\n",
      "{\"prediction\": \" that a comet has struck a young target that was too small and potentially too far out to escape the comet to carry it into the surrounding clouds. The researchers theorised that the object might be made of material that\", \"id\": 10}\r\n",
      "{\"prediction\": \" that the ocean-facing ocean wall of an icy plume may have formed within the past two billion years. It features the first evidence that water could be trapped in a plumescape.\\n\\nThe research comes\", \"id\": 11}\r\n",
      "{\"prediction\": \" that the giant sphinx is an insect from the group Phytosauropterygus. They also found that it's likely related in some way to a spider's mouth.\\n\\nThe creature's\", \"id\": 12}\r\n",
      "{\"prediction\": \" a new species \\u2014 the \\\"Haparovian\\\" species.\\n\\n\\\"This is such a big deal. It would be a significant discovery even if it wasn't,\\\" said Alexander Bozhirov,\", \"id\": 13}\r\n",
      "{\"prediction\": \" a tiny clump of tiny cells in their bodies. This unique type of genetic material is not only very rare, but is also extremely difficult to synthesize, at least in the small doses that might be needed.\", \"id\": 14}\r\n",
      "{\"prediction\": \" The fact that she was killed by terrorists or any other type of terrorist is a crime,\\\" Trump said.\", \"id\": 15}\r\n",
      "{\"prediction\": \" He's the kind who walks me to walks to the hospital, he talks about having to look me well every time I come to have a look. If you're sitting right beside him, who would you ask someone to walk\", \"id\": 16}\r\n",
      "{\"prediction\": \" He is a good guy. I'm all for that. The biggest question is which other team I watch. Would I rather watch their season or not, which team would I instead watch and where would you feel to do\", \"id\": 17}\r\n",
      "{\"prediction\": \" I'd like to start a dialogue with her.\\\"\\n\\n\\\"Yeah? Then what about getting her to come back to your family?\\\" Kelly asks.\\n. He tries to keep his voice low, but that doesn't\", \"id\": 18}\r\n",
      "{\"prediction\": \" This is true. When I saw his house in the backyard, I looked at my camera, and everything was real. He was like crazy. I had no idea how real these things were. But I knew, man \\u2014\", \"id\": 19}\r\n",
      "{\"prediction\": \" The place gets better and better. Now people will have more respect because they don't feel bad about it. When I got here I was a bit confused. I asked if there were people here on Saturday nights? My friends\", \"id\": 20}\r\n",
      "{\"prediction\": \" She is very beautiful. You can see she has quite a personality. We spend a lot of time together on the road. My father always took care of her. That is where it gets really, really interesting, but\", \"id\": 21}\r\n",
      "{\"prediction\": \" I'm a very good dad (to my youngest son) so I could get his dad an apartment.\\\"\\n\\nDietary habits of the family are simple, he says. \\\"I'm very careful with salt. One\", \"id\": 22}\r\n",
      "{\"prediction\": \" He's my brother. I am a family friend. But I was the first to take up this issue. We are close.\\n\\nIn 2006, the City of Denver became the largest employer in the country with\", \"id\": 23}\r\n",
      "{\"prediction\": \" We'd like to believe that he's just the latest in a line of criminals who are trying to break into homes and steal something. He's not a typical criminal.\\\"\\n\\nAccording to the affidavit, \\\"The man\", \"id\": 24}\r\n"
     ]
    }
   ],
   "source": [
    "cat 'same_token_suite_predictions.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading test results from file\n",
    "TestSuite has a `run_from_file()` function that reads the predictions line by line from a file. The `format_fn` parameter is used to parse each line of the file. Our format function, `read_json_prediction()`, converts the JSON object into a tuple of `(predicted text, confidence)`. We don't care about confidence values here, so we will just set confidence to 1 for every prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_prediction(x):\n",
    "    test_output = json.loads(x)\n",
    "    return (test_output['prediction'], 1)\n",
    "suite.run_from_file('same_token_suite_predictions.json', format_fn = read_json_prediction, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same Token Prediction\n",
      "\n",
      "Same animal in response\n",
      "Test cases:      4\n",
      "Fails (rate):    1 (25.0%)\n",
      "\n",
      "Example fails:\n",
      "Prompt:      The aardvark is running in the zoo\n",
      "Completion:      , as he runs out during a lecture and gets caught up in it as well. He also finds out about the future of his mother.\n",
      "\n",
      "Notes\n",
      "----\n",
      "\n",
      "\n",
      "Same country in response\n",
      "Test cases:      10\n",
      "Fails (rate):    10 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "Prompt:      Earlier today, scientists from Namibia discovered\n",
      "Completion:       that a comet has struck a young target that was too small and potentially too far out to escape the comet to carry it into the surrounding clouds. The researchers theorised that the object might be made of material that\n",
      "----\n",
      "Prompt:      Earlier today, scientists from Ukraine discovered\n",
      "Completion:       a new species — the \"Haparovian\" species.\n",
      "\n",
      "\"This is such a big deal. It would be a significant discovery even if it wasn't,\" said Alexander Bozhirov,\n",
      "----\n",
      "Prompt:      Earlier today, scientists from Malaysia discovered\n",
      "Completion:       that the ocean-facing ocean wall of an icy plume may have formed within the past two billion years. It features the first evidence that water could be trapped in a plumescape.\n",
      "\n",
      "The research comes\n",
      "----\n",
      "\n",
      "\n",
      "Same person in response\n",
      "Test cases:      10\n",
      "Fails (rate):    10 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "Prompt:      Marie is my neighbor.\n",
      "Completion:       The fact that she was killed by terrorists or any other type of terrorist is a crime,\" Trump said.\n",
      "----\n",
      "Prompt:      Lawrence is my neighbor.\n",
      "Completion:       We'd like to believe that he's just the latest in a line of criminals who are trying to break into homes and steal something. He's not a typical criminal.\"\n",
      "\n",
      "According to the affidavit, \"The man\n",
      "----\n",
      "Prompt:      Ben is my neighbor.\n",
      "Completion:       He's the kind who walks me to walks to the hospital, he talks about having to look me well every time I come to have a look. If you're sitting right beside him, who would you ask someone to walk\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "suite.summary(format_example_fn = format_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait as we prepare the table data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b92b09b92ab45588bfa5a5d41243299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SuiteSummarizer(stats={'npassed': 0, 'nfailed': 0, 'nfiltered': 0}, test_infos=[{'name': 'Same animal in respo…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "suite.visual_summary_table()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
