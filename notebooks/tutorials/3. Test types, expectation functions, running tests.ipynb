{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import checklist\n",
    "from checklist.editor import Editor\n",
    "from checklist.perturb import Perturb\n",
    "from checklist.test_types import MFT, INV, DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial, we will assume that our task is sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "editor = Editor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimum Functionality Test (MFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Minimum Functionality Test is like a unit test in Software Engineering.\n",
    "If you are testing a certain capability (e.g. 'can the model handle negation?'), an MFT is composed of simple examples that verify a specific behavior.  \n",
    "Let's create a very simple MFT for negations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'easy, academic, ordinary, educational, average, enjoyable, entertaining, interesting, old, independent, art, exciting, good, original, ideal, innocent, excellent, adventure, amateur, awards, introductory, actual, engaging, obscure, experimental, amazing, bad, awful, accessible, intelligent'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, let's find some positive and negative adjectives\n",
    "', '.join(editor.suggest('This is not {a:mask} {thing}.', thing=['book', 'movie', 'show', 'game'])[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = ['good', 'enjoyable', 'exciting', 'excellent', 'amazing', 'great', 'engaging']\n",
    "neg = ['bad', 'terrible', 'awful', 'horrible']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create some data with both positive and negative negations, assuming `1` means positive and `0` means negative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = editor.template('This is not {a:pos} {mask}.', pos=pos, labels=0, save=True, nsamples=100)\n",
    "ret += editor.template('This is not {a:neg} {mask}.', neg=neg, labels=1, save=True, nsamples=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily turn this data into an MFT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = MFT(ret.data, labels=ret.labels, name='Simple negation',\n",
    "           capability='Negation', description='Very simple negations.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `ret` is a dict where keys have the right names for test arguments, we can also use a simpler call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = MFT(**ret, name='Simple negation',\n",
    "           capability='Negation', description='Very simple negations.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use an off-the-shelf sentiment analysis model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pattern.en import sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def predict_proba(inputs):\n",
    "    p1 = np.array([(sentiment(x)[0] + 1)/2. for x in inputs]).reshape(-1, 1)\n",
    "    p0 = 1- p1\n",
    "    return np.hstack((p0, p1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.15, 0.85],\n",
       "       [0.85, 0.15]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predictions are random\n",
    "predict_proba(['good', 'bad'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways of running tests.  \n",
    "In the first (and simplest) way, you pass a function as argument to `test.run`, which gets called to make predictions.  \n",
    "We assume that the function returns a tuple with `(predictions, confidences)`, so we have a wrapper to turn softmax (like our function above) into this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from checklist.pred_wrapper import PredictorWrapper\n",
    "wrapped_pp = PredictorWrapper.wrap_softmax(predict_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1]), array([[0.15, 0.85]]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_pp(['good'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have this function, running the test is as simple as calling `test.run`.  \n",
    "You can run the test on a subset of testcases (for speed's sake) by specifying `n` if needed.  \n",
    "We won't do that here since our test is small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 100 examples\n"
     ]
    }
   ],
   "source": [
    "test.run(wrapped_pp, overwrite = True, n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you run a test, you can print a summary of the results with `test.summary()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cases:      200\n",
      "Test cases run:  100\n",
      "Fails (rate):    41 (41.0%)\n",
      "\n",
      "Example fails:\n",
      "0.7 This is not an exciting moment.\n",
      "----\n",
      "0.0 This is not an awful letter.\n",
      "----\n",
      "0.8 This is not an enjoyable place.\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "test.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that this off-the-shelf system has trouble with negation.\n",
    "Note the failures: examples that should be negative are predicted as positive and vice versa (the number shown is the probability of positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using jupyter notebooks, you can use `test.visual_summary()` for a nice visualization version of these results:  \n",
    "(I'll load a gif so you can see this in preview mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da65f8e7864442159682ec2ad7f11d8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TestSummarizer(stats={'npassed': 59, 'nfailed': 41, 'nfiltered': 0}, summarizer={'name': 'Simple negation', 'd…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from IPython.display import HTML, Image\n",
    "# with open('visual_summary.gif','rb') as f:\n",
    "#     display(Image(data=f.read(), format='png'))\n",
    "test.visual_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second way to run a test is from a prediction file.  \n",
    "First, we export the test into a text file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_raw_file('/tmp/raw_file.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is not an amazing perspective.\r\n",
      "This is not a great show.\r\n",
      "This is not an amazing article.\r\n",
      "This is not an engaging website.\r\n",
      "This is not an enjoyable deal.\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "cat /tmp/raw_file.txt | head -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you get predictions from the examples in the raw file (in order) however you want, and save them in a prediction file.  \n",
    "Let's simulate this process here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = open('/tmp/raw_file.txt').read().splitlines()\n",
    "preds = predict_proba(docs)\n",
    "f = open('/tmp/softmax_preds.txt', 'w')\n",
    "for p in preds:\n",
    "    f.write('%f %f\\n' % tuple(p))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.200000 0.800000\r\n",
      "0.700000 0.300000\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "cat /tmp/softmax_preds.txt | head -n 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run the test from this file.  \n",
    "We have to specify the file format (see the API for possible choices), or a function that takes a line in the file and outputs predictions and confidences.  \n",
    "Since we had already run this test, we have to set `overwrite=True` to overwrite the previous results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.run_from_file('/tmp/softmax_preds.txt', file_format='softmax', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cases:      200\n",
      "Fails (rate):    94 (47.0%)\n",
      "\n",
      "Example fails:\n",
      "0.8 This is not an amazing perspective.\n",
      "----\n",
      "0.8 This is not an amazing substitute.\n",
      "----\n",
      "0.7 This is not an exciting day.\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "test.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invariance tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Invariance test (INV) is when we apply label-preserving perturbations to inputs and expect the model prediction to remain the same.  \n",
    "Let's start by creating a fictitious dataset to serve as an example, and process it with spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ['This was a very nice movie directed by John Smith.',\n",
    "           'Mary Keen was brilliant.', \n",
    "          'I hated everything about this.',\n",
    "          'This movie was very bad.',\n",
    "          'I really liked this movie.',\n",
    "          'just bad.',\n",
    "          'amazing.',\n",
    "          ]\n",
    "pdataset = list(nlp.pipe(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply a simple perturbation: changing people's names and expecting predictions to remain the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This was a very nice movie directed by John Smith.\n",
      "This was a very nice movie directed by Michael Hernandez.\n",
      "This was a very nice movie directed by Christopher Green.\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "t = Perturb.perturb(pdataset, Perturb.change_names)\n",
    "print('\\n'.join(t.data[0][:3]))\n",
    "print('...')\n",
    "test = INV(**t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 22 examples\n",
      "Test cases:      2\n",
      "Fails (rate):    0 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "test.run(wrapped_pp)\n",
    "test.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a different test: adding typos and expecting predictions to remain the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This was a very nice movie directed by John Smith.\n",
      "This was a very nice movie directe dby John Smith.\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "t = Perturb.perturb(dataset, Perturb.add_typos)\n",
    "print('\\n'.join(t.data[0][:3]))\n",
    "print('...')\n",
    "test = INV(**t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 14 examples\n",
      "Test cases:      7\n",
      "Fails (rate):    3 (42.9%)\n",
      "\n",
      "Example fails:\n",
      "0.8 amazing.\n",
      "0.5 amaznig.\n",
      "\n",
      "----\n",
      "0.8 I really liked this movie.\n",
      "0.5 I reall yliked this movie.\n",
      "\n",
      "----\n",
      "0.0 This movie was very bad.\n",
      "0.6 This movie was very ba.d\n",
      "\n",
      "----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bd67cd15fbc41e38070f9ceb0de244d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TestSummarizer(stats={'npassed': 4, 'nfailed': 3, 'nfiltered': 0}, summarizer={'name': None, 'description': No…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test.run(wrapped_pp, overwrite=True)\n",
    "test.summary()\n",
    "test.visual_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directional Expectation tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Directional Expectation test (DIR) is just like an INV, in the sense that we apply a perturbation to existing inputs. However, instead of expecting invariance, we expect the model to behave in a some specified way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let's start with a very simple perturbation: we'll add very negative phrases to the end of our small dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_negative(x):\n",
    "    phrases = ['Anyway, I thought it was bad.', 'Having said this, I hated it', 'The director should be fired.']\n",
    "    return ['%s %s' % (x, p) for p in phrases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('This was a very nice movie directed by John Smith.',\n",
       " ['This was a very nice movie directed by John Smith. Anyway, I thought it was bad.',\n",
       "  'This was a very nice movie directed by John Smith. Having said this, I hated it',\n",
       "  'This was a very nice movie directed by John Smith. The director should be fired.'])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0], add_negative(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would we expect after this perturbation? I think the least we should expect is that the prediction probability of positive should **not go up** (that is, it should be monotonically decreasing).  \n",
    "Monotonicity is an expectation function that is built in, so we don't need to implement it.\n",
    "`tolerance=0.1` means we won't consider it a failure if the prediction probability goes up by less than 0.1, only if it goes up by more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from checklist.expect import Expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "monotonic_decreasing = Expect.monotonic(label=1, increasing=False, tolerance=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Perturb.perturb(dataset, add_negative)\n",
    "test = DIR(**t, expect=monotonic_decreasing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 28 examples\n",
      "Test cases:      7\n",
      "After filtering: 6 (85.7%)\n",
      "Fails (rate):    0 (0.0%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "042fa93b04b24231862956eb826c0539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TestSummarizer(stats={'npassed': 6, 'nfailed': 0, 'nfiltered': 0}, summarizer={'name': None, 'description': No…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test.run(wrapped_pp, overwrite=True)\n",
    "test.summary()\n",
    "test.visual_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing custom expectation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are writing a custom expectation functions, it must return a float or bool for each example such that:\n",
    "- `> 0` (or True) means passed,\n",
    "- `<= 0` or False means fail, and (optionally) the magnitude of the failure, indicated by distance from 0, e.g. -10 is worse than -1\n",
    "- `None` means the test does not apply, and this should not be counted\n",
    "\n",
    "Each test case can have multiple examples. In our MFTs, each test case only had a single example, but in our INVs and DIRs, they had multiple examples (e.g. we changed people's names to various other names).\n",
    "\n",
    "You can write custom expectation functions at multiple levels of granularity.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expectation on a single example\n",
    "\n",
    "If you want to write an expectation function that acts on each individual example, you write a function with the following signature:\n",
    "\n",
    "`def fn(x, pred, conf, label=None, meta=None):`\n",
    "\n",
    "For example, let's write a (useless) expectation function that checks that every prediction confidence is higher than 0.95:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that expects prediction confidence to always be more than 0.9\n",
    "def high_confidence(x, pred, conf, label=None, meta=None):\n",
    "    return conf.max() > 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then wrap this function with `Expect.single`, and apply it to our previous test to see the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "expect_fn = Expect.single(high_confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cases:      7\n",
      "Fails (rate):    7 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "0.8 amazing.\n",
      "0.5 amazing. Anyway, I thought it was bad.\n",
      "0.4 amazing. Having said this, I hated it\n",
      "\n",
      "----\n",
      "0.9 Mary Keen was brilliant.\n",
      "0.6 Mary Keen was brilliant. Anyway, I thought it was bad.\n",
      "0.5 Mary Keen was brilliant. Having said this, I hated it\n",
      "\n",
      "----\n",
      "0.9 This was a very nice movie directed by John Smith.\n",
      "0.5 This was a very nice movie directed by John Smith. Anyway, I thought it was bad.\n",
      "0.5 This was a very nice movie directed by John Smith. Having said this, I hated it\n",
      "\n",
      "----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d42d068bae6c4ceb9684dc8acac6e69f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TestSummarizer(stats={'npassed': 0, 'nfailed': 7, 'nfiltered': 0}, summarizer={'name': None, 'description': No…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test.set_expect(expect_fn)\n",
    "test.summary()\n",
    "test.visual_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that every test case fails now: there is always some prediction in it that has confidence smaller than 0.95.  \n",
    "By default, the way we aggregate all results in a test case is such that the testcase fails if **any** examples in it fail (for MFTs), or **any but the first** fail for INVs and DIRs (because the first is usually the original data point before perturbation). You can change these defaults with the `agg_fn` argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expectation on  pairs \n",
    "\n",
    "Most of the time for DIRs, you want to write an expectation function that acts on pairs of `(original, new)` examples - that is, the original example and the perturbed examples. If this is the case, the signature is as follows:\n",
    "\n",
    "`def fn(orig_pred, pred, orig_conf, conf, labels=None, meta=None)`\n",
    "\n",
    "For example, let's write an expectation function that checks that the prediction **changed** after applying the perturbation, and wrap it with `Expect.pairwise`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def changed_pred(orig_pred, pred, orig_conf, conf, labels=None, meta=None):\n",
    "    return pred != orig_pred\n",
    "expect_fn = Expect.pairwise(changed_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's actually create a new test where we add negation to our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['This was a very nice movie directed by John Smith.',\n",
       "  'This was not a very nice movie directed by John Smith.'],\n",
       " ['Mary Keen was brilliant.', 'Mary Keen was not brilliant.']]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = Perturb.perturb(pdataset, Perturb.add_negation)\n",
    "t.data[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 10 examples\n",
      "Test cases:      5\n",
      "Fails (rate):    1 (20.0%)\n",
      "\n",
      "Example fails:\n",
      "0.8 I really liked this movie.\n",
      "0.6 I really didn't like this movie.\n",
      "\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "test = DIR(**t, expect=expect_fn)\n",
    "test.run(wrapped_pp)\n",
    "test.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the failure: prediction did not change after adding negation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can write much more complex expectation functions, but these are enough for this tutorial.  \n",
    "You can check out `expect.py` or the notebooks for Sentiment Analysis, QQP and SQuAD for many additional examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
